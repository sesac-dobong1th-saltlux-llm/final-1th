{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY7J-CQM3k88"
      },
      "source": [
        "### GPTScore를 통한 성능 평가"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Eqx16q3k9F"
      },
      "source": [
        "[Source](https://github.com/jinlanfu/GPTScore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SwHX3eB3k9G"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('/home/elicer/model/inno/summary/new_df.csv')\n",
        "data.drop(['Unnamed: 0'], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXW8rPYn3k9J",
        "outputId": "7fb32540-8ab1-4781-904b-b97f69f273e0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_summary</th>\n",
              "      <th>Kobart-summary</th>\n",
              "      <th>T5-summary</th>\n",
              "      <th>openai_chain_question_text</th>\n",
              "      <th>openai_chain_question_text_summary</th>\n",
              "      <th>openai_chain_question_kobart_summary</th>\n",
              "      <th>openai_chain_question_T5_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 저는 남을 잘 따르는 팔로...</td>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 저는 넓은 포용성을 가지고...</td>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 저는 팔로워 형으로, 팀장...</td>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 협업에서 가장 중요하게 생...</td>\n",
              "      <td>1. 팔로워 형태의 리더십에 대해 어떻게 생각하시나요?\\n2. 어떤 상황에서 리더 ...</td>\n",
              "      <td>1. RND 직무의 인성면접에 참가한 신입 지원자입니다. 한 명 한 명의 개개인을 ...</td>\n",
              "      <td>1. RND 직무의 인성면접에 참가한 경험이 있는가요?\\n2. 지원자로서 팀장을 서...</td>\n",
              "      <td>1. 협업에서 가장 중요하게 생각하는 것은 무엇입니까?\\n2. 인성면접에서 어떤 키...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 네 협업을 할 때는 제일 ...</td>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 협업을 할 때는 각자의 의...</td>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 네 협업을 할 때 제일 어...</td>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 대학생 시절 화성에 위치한...</td>\n",
              "      <td>1. 협업 시 기준점 설정의 중요성에 대해 어떻게 생각하시나요?\\n2. 협업에서의 ...</td>\n",
              "      <td>1. 협업 시 원칙을 세우는 것이 중요한가요?\\n2. 협업 시 양쪽 다 이익을 낼 ...</td>\n",
              "      <td>1. 협업 시 어려운 점은 무엇입니까?\\n2. 협업을 위해 필요한 기준점은 무엇이라...</td>\n",
              "      <td>1. 화성 공장 견학 경험에 대해 어떤 부분이 가장 기억에 남았나요?\\n2. 생산과...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 저는 시간이 남을 때 주로...</td>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 꼼꼼하지 못한 성격인 저는...</td>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 꼼꼼하지 못한 성격이라 일...</td>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 이전 회사에서 평사원임에도...</td>\n",
              "      <td>1. 저의 시간 관리 방법은 무엇입니까?\\n2. 일정을 정리하면 어떤 이점을 느끼십...</td>\n",
              "      <td>1. RND 직무의 인성면접에 참가한 신입 지원자가 꼼꼼하지 못한 성격이라고 설명했...</td>\n",
              "      <td>1. RND 직무의 인성면접에 참가한 신입 지원자가 꼼꼼하지 못한 성격이라고 말했습...</td>\n",
              "      <td>1. 이전 회사에서 어떤 업무를 맡았나요?\\n2. 어떤 경험을 통해 목표를 달성했나...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 어 네 저는 어 대학 시절...</td>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 대학 시절에 동아리에서 아...</td>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 대학 시절 동아리에서 지역...</td>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 협업을 하면서 가장 어려웠...</td>\n",
              "      <td>1. 어떤 프로젝트를 통해 아이들에게 교육적인 지식을 전달했나요?\\n2. 어떤 효과...</td>\n",
              "      <td>1. 대학 시절에 어떤 종류의 요리 프로그램을 아동들을 대상으로 진행했나요?\\n2....</td>\n",
              "      <td>1. 대학 시절 동아리에서 지역아동센터 아동들을 대상으로 요리 프로그램을 진행한 경...</td>\n",
              "      <td>1. 동료와의 의견 조율이 어려웠던 경험을 어떻게 극복했나요?\\n2. 상대방의 의견...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 제가 인생을 살면서 무언가...</td>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 해외에 처음 나갔을 때 영...</td>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 제가 인생을 살면서 무언가...</td>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 피엠피는 프로젝트 매니저를...</td>\n",
              "      <td>1. 해외에서의 어려움을 극복하기 위해 어떤 노력을 했나요?\\n2. 어떤 결과를 얻...</td>\n",
              "      <td>1. 해외 국제학교에서 영어를 배우며 노력한 경험에 대해 어떤 동기가 있었나요?\\n...</td>\n",
              "      <td>1. 해외에서의 영어 학습 경험에 대해 어떤 도전이 있었나요?\\n2. 어떤 마음가짐...</td>\n",
              "      <td>1. RND 직무의 인성면접에 참가한 신입 지원자가 피엠피에서 어떤 성과를 얻었는지...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  \\\n",
              "0  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 저는 남을 잘 따르는 팔로...   \n",
              "1  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 네 협업을 할 때는 제일 ...   \n",
              "2  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 저는 시간이 남을 때 주로...   \n",
              "3  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 어 네 저는 어 대학 시절...   \n",
              "4  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 제가 인생을 살면서 무언가...   \n",
              "\n",
              "                                        text_summary  \\\n",
              "0  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 저는 넓은 포용성을 가지고...   \n",
              "1  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 협업을 할 때는 각자의 의...   \n",
              "2  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 꼼꼼하지 못한 성격인 저는...   \n",
              "3  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 대학 시절에 동아리에서 아...   \n",
              "4  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 해외에 처음 나갔을 때 영...   \n",
              "\n",
              "                                      Kobart-summary  \\\n",
              "0  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 저는 팔로워 형으로, 팀장...   \n",
              "1  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 네 협업을 할 때 제일 어...   \n",
              "2  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 꼼꼼하지 못한 성격이라 일...   \n",
              "3  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 대학 시절 동아리에서 지역...   \n",
              "4  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 제가 인생을 살면서 무언가...   \n",
              "\n",
              "                                          T5-summary  \\\n",
              "0  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 협업에서 가장 중요하게 생...   \n",
              "1  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 대학생 시절 화성에 위치한...   \n",
              "2  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 이전 회사에서 평사원임에도...   \n",
              "3  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 협업을 하면서 가장 어려웠...   \n",
              "4  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 피엠피는 프로젝트 매니저를...   \n",
              "\n",
              "                          openai_chain_question_text  \\\n",
              "0  1. 팔로워 형태의 리더십에 대해 어떻게 생각하시나요?\\n2. 어떤 상황에서 리더 ...   \n",
              "1  1. 협업 시 기준점 설정의 중요성에 대해 어떻게 생각하시나요?\\n2. 협업에서의 ...   \n",
              "2  1. 저의 시간 관리 방법은 무엇입니까?\\n2. 일정을 정리하면 어떤 이점을 느끼십...   \n",
              "3  1. 어떤 프로젝트를 통해 아이들에게 교육적인 지식을 전달했나요?\\n2. 어떤 효과...   \n",
              "4  1. 해외에서의 어려움을 극복하기 위해 어떤 노력을 했나요?\\n2. 어떤 결과를 얻...   \n",
              "\n",
              "                  openai_chain_question_text_summary  \\\n",
              "0  1. RND 직무의 인성면접에 참가한 신입 지원자입니다. 한 명 한 명의 개개인을 ...   \n",
              "1  1. 협업 시 원칙을 세우는 것이 중요한가요?\\n2. 협업 시 양쪽 다 이익을 낼 ...   \n",
              "2  1. RND 직무의 인성면접에 참가한 신입 지원자가 꼼꼼하지 못한 성격이라고 설명했...   \n",
              "3  1. 대학 시절에 어떤 종류의 요리 프로그램을 아동들을 대상으로 진행했나요?\\n2....   \n",
              "4  1. 해외 국제학교에서 영어를 배우며 노력한 경험에 대해 어떤 동기가 있었나요?\\n...   \n",
              "\n",
              "                openai_chain_question_kobart_summary  \\\n",
              "0  1. RND 직무의 인성면접에 참가한 경험이 있는가요?\\n2. 지원자로서 팀장을 서...   \n",
              "1  1. 협업 시 어려운 점은 무엇입니까?\\n2. 협업을 위해 필요한 기준점은 무엇이라...   \n",
              "2  1. RND 직무의 인성면접에 참가한 신입 지원자가 꼼꼼하지 못한 성격이라고 말했습...   \n",
              "3  1. 대학 시절 동아리에서 지역아동센터 아동들을 대상으로 요리 프로그램을 진행한 경...   \n",
              "4  1. 해외에서의 영어 학습 경험에 대해 어떤 도전이 있었나요?\\n2. 어떤 마음가짐...   \n",
              "\n",
              "                    openai_chain_question_T5_summary  \n",
              "0  1. 협업에서 가장 중요하게 생각하는 것은 무엇입니까?\\n2. 인성면접에서 어떤 키...  \n",
              "1  1. 화성 공장 견학 경험에 대해 어떤 부분이 가장 기억에 남았나요?\\n2. 생산과...  \n",
              "2  1. 이전 회사에서 어떤 업무를 맡았나요?\\n2. 어떤 경험을 통해 목표를 달성했나...  \n",
              "3  1. 동료와의 의견 조율이 어려웠던 경험을 어떻게 극복했나요?\\n2. 상대방의 의견...  \n",
              "4  1. RND 직무의 인성면접에 참가한 신입 지원자가 피엠피에서 어떤 성과를 얻었는지...  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV6Pe3gT3k9L"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import sys\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import openai\n",
        "\n",
        "import time\n",
        "import sys\n",
        "import pandas as pd\n",
        "import openai\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "class GPT3Model():\n",
        "\n",
        "    def __init__(self, model_name, api_key):\n",
        "        openai.api_key = api_key\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
        "\n",
        "    def do_inference(self, input, output, max_length=2048):\n",
        "        losses = []\n",
        "        data = input + output\n",
        "\n",
        "        response = self.gpt3(data)\n",
        "        out = response[\"choices\"][0]\n",
        "\n",
        "        assert input + output == out[\"text\"]\n",
        "        i = 0\n",
        "        # find the end position of the input...\n",
        "        i = out['logprobs']['text_offset'].index(len(input) - 1)\n",
        "        if i == 0:\n",
        "            i = i + 1\n",
        "\n",
        "        print('eval text', out['logprobs']['tokens'][i: -1])\n",
        "        loss = -sum(out['logprobs'][\"token_logprobs\"][i:-1])  # ignore the last '.'\n",
        "        avg_loss = loss / (len(out['logprobs']['text_offset']) - i - 1)  # 1 is the last '.'\n",
        "        print('avg_loss: ', avg_loss)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        return avg_loss\n",
        "\n",
        "\n",
        "def gpt3(prompt, model_name, api_key, max_len=100, temp=0.7):\n",
        "\n",
        "    messages = [{\"role\":\"user\",\"content\":prompt}]\n",
        "\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model = model_name,\n",
        "            messages = messages,\n",
        "            max_tokens = max_len,\n",
        "            temperature = temp,\n",
        "        )\n",
        "    except openai.error as e:\n",
        "        print(e)\n",
        "        return None\n",
        "\n",
        "    if response is not None and \"choices\" in response:\n",
        "        return response[\"choices\"][0]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def gpt3score(input, output, model_name, api_key):\n",
        "\n",
        "    metaicl_model = GPT3Model(model_name, api_key)\n",
        "    avg_loss = metaicl_model.do_inference(input, output)\n",
        "    score = -avg_loss\n",
        "    return score\n",
        "\n",
        "\n",
        "def calculate_gpt3_scores_from_dataframe(data, gpt3_model='gpt-3.5-turbo-16k', api_key=None, max_tokens=1024):\n",
        "    openai.api_key = api_key  # OpenAI API 키 설정\n",
        "    data['text_gpt3_score'] = 0\n",
        "    data['text_summary_gpt3_score'] = 0\n",
        "    data['kobart_summary_gpt3_score'] = 0\n",
        "    data['T5_summary_gpt3_score'] = 0\n",
        "\n",
        "    for index, row in data.iterrows():\n",
        "        text = row['text']\n",
        "        text_summary = row['text_summary']\n",
        "        kobart_summary = row['Kobart-summary']\n",
        "        T5_summary = row['T5-summary']\n",
        "        openai_chain_question_text = row['openai_chain_question_text']\n",
        "        openai_chain_question_text_summary = row['openai_chain_question_text_summary']\n",
        "        openai_chain_question_kobart_summary = row['openai_chain_question_kobart_summary']\n",
        "        openai_chain_question_T5_summary = row['openai_chain_question_T5_summary']\n",
        "\n",
        "        try:\n",
        "            data.at[index, 'text_gpt3_score'] = gpt3score(text, openai_chain_question_text, gpt3_model, api_key, max_tokens)\n",
        "            data.at[index, 'text_summary_gpt3_score'] = gpt3score(text_summary, openai_chain_question_text_summary, gpt3_model, api_key, max_tokens)\n",
        "            data.at[index, 'kobart_summary_gpt3_score'] = gpt3score(kobart_summary, openai_chain_question_kobart_summary, gpt3_model, api_key, max_tokens)\n",
        "            data.at[index, 'T5_summary_gpt3_score'] = gpt3score(T5_summary, openai_chain_question_T5_summary, gpt3_model, api_key, max_tokens)\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating GPT-3 scores for row {index}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXsbRBen3k9O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVbR2EfH3k9P",
        "outputId": "79dc4934-a649-4af5-d4ce-6412b3726991"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_summary</th>\n",
              "      <th>Kobart-summary</th>\n",
              "      <th>T5-summary</th>\n",
              "      <th>openai_chain_question_text</th>\n",
              "      <th>openai_chain_question_text_summary</th>\n",
              "      <th>openai_chain_question_kobart_summary</th>\n",
              "      <th>openai_chain_question_T5_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 저는 남을 잘 따르는 팔로...</td>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 저는 넓은 포용성을 가지고...</td>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 저는 팔로워 형으로, 팀장...</td>\n",
              "      <td>RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 협업에서 가장 중요하게 생...</td>\n",
              "      <td>1. 팔로워 형태의 리더십에 대해 어떻게 생각하시나요?\\n2. 어떤 상황에서 리더 ...</td>\n",
              "      <td>1. RND 직무의 인성면접에 참가한 신입 지원자입니다. 한 명 한 명의 개개인을 ...</td>\n",
              "      <td>1. RND 직무의 인성면접에 참가한 경험이 있는가요?\\n2. 지원자로서 팀장을 서...</td>\n",
              "      <td>1. 협업에서 가장 중요하게 생각하는 것은 무엇입니까?\\n2. 인성면접에서 어떤 키...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  \\\n",
              "0  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 저는 남을 잘 따르는 팔로...   \n",
              "\n",
              "                                        text_summary  \\\n",
              "0  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 저는 넓은 포용성을 가지고...   \n",
              "\n",
              "                                      Kobart-summary  \\\n",
              "0  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 저는 팔로워 형으로, 팀장...   \n",
              "\n",
              "                                          T5-summary  \\\n",
              "0  RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 협업에서 가장 중요하게 생...   \n",
              "\n",
              "                          openai_chain_question_text  \\\n",
              "0  1. 팔로워 형태의 리더십에 대해 어떻게 생각하시나요?\\n2. 어떤 상황에서 리더 ...   \n",
              "\n",
              "                  openai_chain_question_text_summary  \\\n",
              "0  1. RND 직무의 인성면접에 참가한 신입 지원자입니다. 한 명 한 명의 개개인을 ...   \n",
              "\n",
              "                openai_chain_question_kobart_summary  \\\n",
              "0  1. RND 직무의 인성면접에 참가한 경험이 있는가요?\\n2. 지원자로서 팀장을 서...   \n",
              "\n",
              "                    openai_chain_question_T5_summary  \n",
              "0  1. 협업에서 가장 중요하게 생각하는 것은 무엇입니까?\\n2. 인성면접에서 어떤 키...  "
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeNazhzR3k9R",
        "outputId": "33406cb5-59e3-407a-895b-2416e1d87185"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error calculating GPT-3 scores for row 0: gpt3score() takes 4 positional arguments but 5 were given\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>text_summary</th>\n",
              "      <th>Kobart-summary</th>\n",
              "      <th>T5-summary</th>\n",
              "      <th>openai_chain_question_text</th>\n",
              "      <th>openai_chain_question_text_summary</th>\n",
              "      <th>openai_chain_question_kobart_summary</th>\n",
              "      <th>openai_chain_question_T5_summary</th>\n",
              "      <th>text_gpt3_score</th>\n",
              "      <th>text_summary_gpt3_score</th>\n",
              "      <th>kobart_summary_gpt3_score</th>\n",
              "      <th>T5_summary_gpt3_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>여기에 본문 텍스트를 입력합니다</td>\n",
              "      <td>본문 텍스트의 요약문을 입력합니다</td>\n",
              "      <td>코바트 요약문</td>\n",
              "      <td>T5 요약문</td>\n",
              "      <td>본문을 요약하면?</td>\n",
              "      <td>요약문을 설명하면?</td>\n",
              "      <td>코바트 요약문을 설명하면?</td>\n",
              "      <td>T5 요약문을 설명하면?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                text        text_summary Kobart-summary T5-summary  \\\n",
              "0  여기에 본문 텍스트를 입력합니다  본문 텍스트의 요약문을 입력합니다        코바트 요약문     T5 요약문   \n",
              "\n",
              "  openai_chain_question_text openai_chain_question_text_summary  \\\n",
              "0                  본문을 요약하면?                         요약문을 설명하면?   \n",
              "\n",
              "  openai_chain_question_kobart_summary openai_chain_question_T5_summary  \\\n",
              "0                       코바트 요약문을 설명하면?                    T5 요약문을 설명하면?   \n",
              "\n",
              "   text_gpt3_score  text_summary_gpt3_score  kobart_summary_gpt3_score  \\\n",
              "0                0                        0                          0   \n",
              "\n",
              "   T5_summary_gpt3_score  \n",
              "0                      0  "
            ]
          },
          "execution_count": 169,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"여기에 본문 텍스트를 입력합니다\"\n",
        "text_summary = \"본문 텍스트의 요약문을 입력합니다\"\n",
        "kobart_summary = \"코바트 요약문\"\n",
        "t5_summary = \"T5 요약문\"\n",
        "\n",
        "openai_q_text = \"본문을 요약하면?\"\n",
        "openai_q_text_summary = \"요약문을 설명하면?\"\n",
        "openai_q_kobart = \"코바트 요약문을 설명하면?\"\n",
        "openai_q_t5 = \"T5 요약문을 설명하면?\"\n",
        "\n",
        "data = pd.DataFrame([[\n",
        "    text, text_summary, kobart_summary, t5_summary,\n",
        "    openai_q_text, openai_q_text_summary, openai_q_kobart, openai_q_t5\n",
        "]])\n",
        "\n",
        "data.columns = [\n",
        "    \"text\", \"text_summary\", \"Kobart-summary\", \"T5-summary\",\n",
        "    \"openai_chain_question_text\",\n",
        "    \"openai_chain_question_text_summary\",\n",
        "    \"openai_chain_question_kobart_summary\",\n",
        "    \"openai_chain_question_T5_summary\"\n",
        "]\n",
        "\n",
        "calculate_gpt3_scores_from_dataframe(data, api_key=api_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pk9c-ZlC3k9S",
        "outputId": "05fb7ce9-8c56-48c5-8718-9d12c3a8e6cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'index': 0,\n",
              " 'message': {'role': 'assistant', 'content': ''},\n",
              " 'logprobs': 'null',\n",
              " 'finish_reason': 'length'}"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response[\"choices\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFoA31--3k9S",
        "outputId": "4fe423b8-a78b-4539-f05a-7fb0644ff3c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "본문은 어떤 내용이 있는지 간략하게 설명한 것을 말합니다. 요약문은 해당 본문의 주요 내용을 간략하고 요점적으로 정리한 것을 말합니다.\n"
          ]
        }
      ],
      "source": [
        "response = gpt3('본문요약문', api_key)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZ7xDZyt3k9T",
        "outputId": "2d0444a2-6bbf-4382-c2cf-a63580cd0aad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "InvalidRequestError\n",
            "Prompt passed in:\n",
            "\n",
            "RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 저는 남을 잘 따르는 팔로워 형입니다. 물론 상황에 따라 리더의 역할을 수행해야 한다면 기꺼이 리더가 되는 멀티 플레이어가 되고자 하지만 기본적으로는 팀장을 서포트 해주는 역할을 주로 맡고 있습니다. 일단 저는 뒤에서 보조해 주는 것을 잘합니다. 리더나 팀원들이 필요로 하는 자료나 레퍼런스를 가져오거나 멀리서 상황을 객관적으로 지켜보고 정리하는 것을 좋아합니다. 리더는 많은 책임을 갖고 모두를 이끌어야 합니다. 저는 그런 넓은 포용성을 가지고 있기보다는 한 명 한 명의 개개인을 세심하게 관찰하고 커뮤니케이션을 하는 것을 더 잘하는 편입니다. 따라서 저는 모든 팀원들과 화합하는 팔로워 형이라고 소개하겠습니다.1. 팔로워 형태의 리더십에 대해 어떻게 생각하시나요?\n",
            "2. 어떤 상황에서 리더 역할을 맡아본 경험이 있으신가요?\n",
            "3. 팀원들과의 화합을 위해 어떤 노력을 기울이고 계신가요? \n",
            "\n",
            "한국어로 번역:\n",
            "1. 팔로워 형태의 리더십에 대해 어떻게 생각하시나요?\n",
            "2. 어떤 상황에서 리더 역할을 맡아본 경험이 있으신가요?\n",
            "3. 팀원들과의 화합을 위해 어떤 노력을 기울이고 계신가요?\n",
            "\n",
            "\n",
            "InvalidRequestError\n",
            "Prompt passed in:\n",
            "\n",
            "RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 저는 남을 잘 따르는 팔로워 형입니다. 물론 상황에 따라 리더의 역할을 수행해야 한다면 기꺼이 리더가 되는 멀티 플레이어가 되고자 하지만 기본적으로는 팀장을 서포트 해주는 역할을 주로 맡고 있습니다. 일단 저는 뒤에서 보조해 주는 것을 잘합니다. 리더나 팀원들이 필요로 하는 자료나 레퍼런스를 가져오거나 멀리서 상황을 객관적으로 지켜보고 정리하는 것을 좋아합니다. 리더는 많은 책임을 갖고 모두를 이끌어야 합니다. 저는 그런 넓은 포용성을 가지고 있기보다는 한 명 한 명의 개개인을 세심하게 관찰하고 커뮤니케이션을 하는 것을 더 잘하는 편입니다. 따라서 저는 모든 팀원들과 화합하는 팔로워 형이라고 소개하겠습니다.1. 팔로워 형태의 리더십에 대해 어떻게 생각하시나요?\n",
            "2. 어떤 상황에서 리더 역할을 맡아본 경험이 있으신가요?\n",
            "3. 팀원들과의 화합을 위해 어떤 노력을 기울이고 계신가요? \n",
            "\n",
            "한국어로 번역:\n",
            "1. 팔로워 형태의 리더십에 대해 어떻게 생각하시나요?\n",
            "2. 어떤 상황에서 리더 역할을 맡아본 경험이 있으신가요?\n",
            "3. 팀원들과의 화합을 위해 어떤 노력을 기울이고 계신가요?\n",
            "\n",
            "\n",
            "InvalidRequestError\n",
            "Prompt passed in:\n",
            "\n",
            "RND 직무의 인성면접에 참가하게 된 신입 지원자입니다. 저는 남을 잘 따르는 팔로워 형입니다. 물론 상황에 따라 리더의 역할을 수행해야 한다면 기꺼이 리더가 되는 멀티 플레이어가 되고자 하지만 기본적으로는 팀장을 서포트 해주는 역할을 주로 맡고 있습니다. 일단 저는 뒤에서 보조해 주는 것을 잘합니다. 리더나 팀원들이 필요로 하는 자료나 레퍼런스를 가져오거나 멀리서 상황을 객관적으로 지켜보고 정리하는 것을 좋아합니다. 리더는 많은 책임을 갖고 모두를 이끌어야 합니다. 저는 그런 넓은 포용성을 가지고 있기보다는 한 명 한 명의 개개인을 세심하게 관찰하고 커뮤니케이션을 하는 것을 더 잘하는 편입니다. 따라서 저는 모든 팀원들과 화합하는 팔로워 형이라고 소개하겠습니다.1. 팔로워 형태의 리더십에 대해 어떻게 생각하시나요?\n",
            "2. 어떤 상황에서 리더 역할을 맡아본 경험이 있으신가요?\n",
            "3. 팀원들과의 화합을 위해 어떤 노력을 기울이고 계신가요? \n",
            "\n",
            "한국어로 번역:\n",
            "1. 팔로워 형태의 리더십에 대해 어떻게 생각하시나요?\n",
            "2. 어떤 상황에서 리더 역할을 맡아본 경험이 있으신가요?\n",
            "3. 팀원들과의 화합을 위해 어떤 노력을 기울이고 계신가요?\n",
            "\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/home/elicer/model/inno/summary/select_summary_method.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m scores_df \u001b[39m=\u001b[39m calculate_gpt3_scores_from_dataframe(data, gpt3_model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgpt-3.5-turbo-instruct\u001b[39;49m\u001b[39m'\u001b[39;49m, api_key\u001b[39m=\u001b[39;49mopenai_api_key)\n\u001b[1;32m      <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# 결과 확인\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(scores_df\u001b[39m.\u001b[39mhead())\n",
            "\u001b[1;32m/home/elicer/model/inno/summary/select_summary_method.ipynb Cell 24\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m openai_chain_question_T5_summary \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mopenai_chain_question_T5_summary\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m     data\u001b[39m.\u001b[39mat[index, \u001b[39m'\u001b[39m\u001b[39mtext_gpt3_score\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m gpt3score(text, openai_chain_question_text, gpt3_model, api_key, max_tokens)\n\u001b[1;32m     <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m     data\u001b[39m.\u001b[39mat[index, \u001b[39m'\u001b[39m\u001b[39mtext_summary_gpt3_score\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m gpt3score(text_summary, openai_chain_question_text_summary, gpt3_model, api_key, max_tokens)\n\u001b[1;32m     <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m     data\u001b[39m.\u001b[39mat[index, \u001b[39m'\u001b[39m\u001b[39mkobart_summary_gpt3_score\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m gpt3score(kobart_summary, openai_chain_question_kobart_summary, gpt3_model, api_key, max_tokens)\n",
            "\u001b[1;32m/home/elicer/model/inno/summary/select_summary_method.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgpt3score\u001b[39m(\u001b[39minput\u001b[39m, output, gpt3_model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m'\u001b[39m, api_key\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, max_tokens\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m     metaicl_model \u001b[39m=\u001b[39m GPT3Model(gpt3_model, api_key)\n\u001b[0;32m--> <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m     avg_loss \u001b[39m=\u001b[39m metaicl_model\u001b[39m.\u001b[39;49mdo_inference(\u001b[39minput\u001b[39;49m, output, max_tokens)\n\u001b[1;32m    <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m     \u001b[39mif\u001b[39;00m avg_loss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m         score \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mavg_loss\n",
            "\u001b[1;32m/home/elicer/model/inno/summary/select_summary_method.ipynb Cell 24\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m \u001b[39m+\u001b[39m output\n\u001b[0;32m---> <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgpt3(data, max_len\u001b[39m=\u001b[39;49mmax_length)\n\u001b[1;32m     <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m out \u001b[39m=\u001b[39m response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(out)\n",
            "\u001b[1;32m/home/elicer/model/inno/summary/select_summary_method.ipynb Cell 24\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m received:\n\u001b[1;32m     <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m         response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_name,\n\u001b[1;32m     <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m                                         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m     <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m                                         max_tokens\u001b[39m=\u001b[39;49mmax_len,\n\u001b[1;32m     <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m                                         temperature\u001b[39m=\u001b[39;49mtemp,\n\u001b[1;32m     <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m                                         logprobs\u001b[39m=\u001b[39;49mnum_log_probs,\n\u001b[1;32m     <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m                                         echo\u001b[39m=\u001b[39;49mecho,\n\u001b[1;32m     <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m                                         stop\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m                                         n\u001b[39m=\u001b[39;49mn)\n\u001b[1;32m     <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mprompt: \u001b[39m\u001b[39m'\u001b[39m, prompt)\n\u001b[1;32m     <a href='vscode-notebook-cell://dkqeyamejcpwptfe.tunnel-pt.elice.io/home/elicer/model/inno/summary/select_summary_method.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m         received \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_requestor.py:288\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[1;32m    291\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    292\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    293\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    294\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    295\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/openai/api_requestor.py:596\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m     _thread_context\u001b[39m.\u001b[39msession_create_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    595\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    597\u001b[0m         method,\n\u001b[1;32m    598\u001b[0m         abs_url,\n\u001b[1;32m    599\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    600\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    601\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    602\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    603\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    604\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    607\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    790\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    792\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    794\u001b[0m     conn,\n\u001b[1;32m    795\u001b[0m     method,\n\u001b[1;32m    796\u001b[0m     url,\n\u001b[1;32m    797\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    798\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    799\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    800\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    801\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    802\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    803\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    804\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    805\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    806\u001b[0m )\n\u001b[1;32m    808\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    809\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:537\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    538\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:466\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    465\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    468\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1164\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "scores_df = calculate_gpt3_scores_from_dataframe(data, gpt3_model='gpt-3.5-turbo-instruct', api_key=openai_api_key)\n",
        "\n",
        "# 결과 확인\n",
        "print(scores_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pa0cYUx3k9U"
      },
      "outputs": [],
      "source": [
        "data.drop(['text_gpt3_score','text_summary_gpt3_score','kobart_summary_gpt3_score','T5_summary_gpt3_score'], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxhrUpFc3k9U"
      },
      "outputs": [],
      "source": [
        "response = {\n",
        "  \"id\": \"chatcmpl-9H8na6Ms\",\n",
        "  \"object\": \"chat.completion\",\n",
        "  \"created\": 171387,\n",
        "  \"model\": \"gpt-3.5-turbo-16k-0613\",\n",
        "  \"choices\": [\n",
        "    {\n",
        "      \"index\": 0,\n",
        "      \"message\": {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"\"\n",
        "      },\n",
        "      \"logprobs\": 'null',\n",
        "      \"finish_reason\": \"length\"\n",
        "    }\n",
        "  ],\n",
        "  \"usage\": {\n",
        "    \"prompt_tokens\": 31,\n",
        "    \"completion_tokens\": 0,\n",
        "    \"total_tokens\": 31\n",
        "  },\n",
        "  \"system_fingerprint\": 'null'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLFMpsxn3k9U"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import sys\n",
        "import time\n",
        "\n",
        "def gpt3(prompt, api_key, model_name=\"gpt-3.5-turbo\", max_len=100, temp=0.7):\n",
        "\n",
        "    messages = [{\"role\":\"user\",\"content\":prompt}]\n",
        "\n",
        "    response = None\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model = model_name,\n",
        "            messages = messages,\n",
        "            max_tokens = max_len,\n",
        "            temperature = temp,\n",
        "        )\n",
        "    except openai.error.InvalidRequestError as e:\n",
        "        print(f\"InvalidRequestError: {e}\")\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "    except openai.error.APIError as e:\n",
        "        print(f\"APIError: {e}\")\n",
        "        time.sleep(1)\n",
        "\n",
        "    if response is not None and \"choices\" in response:\n",
        "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "    else:\n",
        "        return \"No response received\"\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}